<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>buildstock_query.aggregate_query API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>buildstock_query.aggregate_query</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import sqlalchemy as sa
from sqlalchemy.sql import func as safunc
import datetime
import numpy as np
import logging
import buildstock_query.main as main

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
FUELS = [&#39;electricity&#39;, &#39;natural_gas&#39;, &#39;propane&#39;, &#39;fuel_oil&#39;, &#39;coal&#39;, &#39;wood_cord&#39;, &#39;wood_pellets&#39;]


class BuildStockAggregate:
    &#34;&#34;&#34;A class to perform various aggregation queries for both timeseries and annual results.
    &#34;&#34;&#34;
    def __init__(self, buildstock_query: &#39;main.BuildStockQuery&#39;) -&gt; None:
        self._bsq = buildstock_query

    def aggregate_annual(self,
                         enduses: list[str] | None = None,
                         group_by: list[str] | None = None,
                         sort: bool = False,
                         upgrade_id: str | int | None = None,
                         join_list: list[tuple[str, str, str]] | None = None,
                         weights: list[str | tuple] | None = None,
                         restrict: list[tuple[str, list]] | None = None,
                         get_quartiles: bool = False,
                         run_async: bool = False,
                         get_query_only: bool = False):
        &#34;&#34;&#34;
        Aggregates the baseline annual result on select enduses.
        Check the argument description below to learn about additional features and options.
        Args:
            enduses: The list of enduses to aggregate. Defaults to all electricity enduses

            group_by: The list of columns to group the aggregation by.

            sort: Whether to sort the results by group_by colummns

            upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

            join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                       `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                       tuples.
                       Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                                where baseline_column_name and new_column_name are the columns on which the new_table
                                should be joined to baseline table.

            weights: The additional columns to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.
                     It is specified as either list of string or list of tuples. When only string is used, the string
                     is the column name, when tuple is passed, the second element is the table name.

            restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                      Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
            get_quartiles: If true, return the following quartiles in addition to the sum for each enduses:
                           [0, 0.02, .25, .5, .75, .98, 1]. The 0% quartile is the minimum and the 100% quartile
                           is the maximum.
            run_async: Whether to run the query in the background. Returns immediately if running in background,
                       blocks otherwise.
            get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                            submitting multiple queries or debugging

        Returns:
                if get_query_only is True, returns the query_string, otherwise,
                    if run_async is True, it returns a query_execution_id.
                    if run_async is False, it returns the result_dataframe

        &#34;&#34;&#34;
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []

        [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
        if upgrade_id in {None, 0, &#39;0&#39;}:
            enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;baseline&#39;)
        else:
            upgrade_id = self._bsq._validate_upgrade(upgrade_id)
            enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;upgrade&#39;)

        total_weight = self._bsq._get_weight(weights)
        enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduse_cols]
        if get_quartiles:
            enduse_selection += [sa.func.approx_percentile(enduse, [0, 0.02, 0.25, 0.5, 0.75, 0.98, 1]).label(
                f&#34;{self._bsq._simple_label(enduse.name)}__quartiles&#34;) for enduse in enduse_cols]

        grouping_metrics_selction = [safunc.sum(1).label(&#34;sample_count&#34;),
                                     safunc.sum(total_weight).label(&#34;units_count&#34;)]

        if not group_by:
            query = sa.select(grouping_metrics_selction + enduse_selection)
            group_by_selection = []
        else:
            group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=True)
            query = sa.select(group_by_selection + grouping_metrics_selction + enduse_selection)
        # jj = self.bs_table.join(self.ts_table, self.ts_table.c[&#39;building_id&#39;]==self.bs_table.c[&#39;building_id&#39;])
        # self._compile(query.select_from(jj))
        if upgrade_id not in [None, 0, &#39;0&#39;]:
            tbljoin = self._bsq.bs_table.join(
                self._bsq.up_table, sa.and_(self._bsq.bs_table.c[self._bsq.building_id_column_name] ==
                                            self._bsq.up_table.c[self._bsq.building_id_column_name],
                                            self._bsq.up_table.c[&#34;upgrade&#34;] == str(upgrade_id),
                                            self._bsq.up_table.c[&#34;completed_status&#34;] == &#34;Success&#34;))
            query = query.select_from(tbljoin)

        restrict = [(self._bsq.bs_table.c[&#39;completed_status&#39;], [&#39;Success&#39;])] + restrict
        query = self._bsq._add_join(query, join_list)
        query = self._bsq._add_restrict(query, restrict)
        query = self._bsq._add_group_by(query, group_by_selection)
        query = self._bsq._add_order_by(query, group_by_selection if sort else [])

        if get_query_only:
            return self._bsq._compile(query)

        return self._bsq.execute(query, run_async=run_async)

    def _aggregate_timeseries_light(self,
                                    enduses: list[str] | None = None,
                                    group_by: list[str] | None = None,
                                    sort: bool = False,
                                    join_list: list[tuple[str, str, str]] | None = None,
                                    weights: list[str] | None = None,
                                    restrict: list[tuple[str, list]] | None = None,
                                    run_async: bool = False,
                                    get_query_only: bool = False,
                                    limit=None
                                    ):
        &#34;&#34;&#34;
        Lighter version of aggregate_timeseries where each enduse is submitted as a separate query to be light on
        Athena. For information on the input parameters, check the documentation on aggregate_timeseries.
        &#34;&#34;&#34;
        enduses = list(enduses) if enduses else []
        group_by = list(group_by) if group_by else []
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []

        if run_async:
            raise ValueError(&#34;Async run is not available for aggregate_timeseries_light since it needs to combine&#34;
                             &#34;the result after the query finishes.&#34;)

        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        print(enduses)
        batch_queries_to_submit = []
        for enduse in enduse_cols:
            query = self.aggregate_timeseries(enduses=[enduse.name],
                                              group_by=group_by,
                                              sort=sort,
                                              join_list=join_list,
                                              weights=weights,
                                              restrict=restrict,
                                              get_query_only=True,
                                              limit=limit)
            batch_queries_to_submit.append(query)

        if get_query_only:
            logger.warning(&#34;Not recommended to use get_query_only and split_enduses used together.&#34;
                           &#34; The results from the queries cannot be directly combined to get the desired result.&#34;
                           &#34; There are further processing done in the function. The queries should be used for&#34;
                           &#34; information or debugging purpose only. Use get_query_only=False to get proper result.&#34;)
            return batch_queries_to_submit

        batch_query_id = self._bsq.submit_batch_query(batch_queries_to_submit)

        result_dfs = self._bsq.get_batch_query_result(batch_id=batch_query_id, combine=False)
        logger.info(&#34;Joining the individual enduses result into a single DataFrame&#34;)
        group_by = self._bsq._clean_group_by(group_by)
        for res in result_dfs:
            res.set_index(group_by, inplace=True)
        self.result_dfs = result_dfs
        joined_enduses_df = result_dfs[0].drop(columns=[&#39;query_id&#39;])
        for enduse, res in list(zip(enduses, result_dfs))[1:]:
            joined_enduses_df = joined_enduses_df.join(res[[enduse.name]])

        logger.info(&#34;Joining Completed.&#34;)
        return joined_enduses_df.reset_index()

    def aggregate_timeseries(self,
                             enduses: list[str] | None = None,
                             group_by: list[str] | None = None,
                             upgrade_id: int | None = None,
                             sort: bool = False,
                             join_list: list[tuple[str, str, str]] | None = None,
                             weights: list[str] | None = None,
                             restrict: list[tuple[str, list]] | None = None,
                             run_async: bool = False,
                             split_enduses: bool = False,
                             collapse_ts: bool = False,
                             get_query_only: bool = False,
                             limit: int | None = None
                             ):
        &#34;&#34;&#34;
        Aggregates the timeseries result on select enduses.
        Check the argument description below to learn about additional features and options.
        Args:
            enduses: The list of enduses to aggregate. Defaults to all electricity enduses

            group_by: The list of columns to group the aggregation by.

            upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

            order_by: The columns by which to sort the result.

            join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                       `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                       tuples.
                       Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                                where baseline_column_name and new_column_name are the columns on which the new_table
                                should be joined to baseline table.

            weights: The additional column to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.

            restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                      Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
            limit: The maximum number of rows to query

            run_async: Whether to run the query in the background. Returns immediately if running in background,
                       blocks otherwise.
            split_enduses: Whether to query for each enduses in a separate query to reduce Athena load for query. Useful
                           when Athena runs into &#34;Query exhausted resources ...&#34; errors.
            get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                            submitting multiple queries or debugging


        Returns:
                if get_query_only is True, returns the query_string, otherwise,
                    if run_async is True, it returns a query_execution_id.
                    if run_async is False, it returns the result_dataframe

        &#34;&#34;&#34;
        enduses = list(enduses) if enduses else []
        group_by = list(group_by) if group_by else []
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []
        upgrade_id = self._bsq._validate_upgrade(upgrade_id)

        if split_enduses:
            return self._aggregate_timeseries_light(enduses=enduses, group_by=group_by, sort=sort,
                                                    join_list=join_list, weights=weights, restrict=restrict,
                                                    run_async=run_async, get_query_only=get_query_only,
                                                    limit=limit)
        [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
        enduses_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        total_weight = self._bsq._get_weight(weights)

        enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduses_cols]

        if self._bsq.timestamp_column_name not in group_by and collapse_ts:
            logger.info(&#34;Aggregation done accross timestamps. Result no longer a timeseries.&#34;)
            # The aggregation is done across time so we should correct sample_count and units_count
            rows_per_building = self._bsq._get_rows_per_building()
            grouping_metrics_selection = [(safunc.sum(1) / rows_per_building).label(
                &#34;sample_count&#34;), safunc.sum(total_weight / rows_per_building).label(&#34;units_count&#34;)]
        elif self._bsq.timestamp_column_name not in group_by:
            group_by.append(self._bsq.timestamp_column_name)
            grouping_metrics_selection = [safunc.sum(1).label(
                &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
        elif collapse_ts:
            raise ValueError(&#34;collapse_ts is true, but there is timestamp column in group_by.&#34;)
        else:
            grouping_metrics_selection = [safunc.sum(1).label(
                &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
        group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=False)

        query = sa.select(group_by_selection + grouping_metrics_selection + enduse_selection)
        query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
        if join_list:
            query = self._bsq._add_join(query, join_list)

        group_by_names = [g.name for g in group_by_selection]
        upgrade_in_restrict = any(entry[0] == &#39;upgrade&#39; for entry in restrict)
        if self._bsq.up_table is not None and not upgrade_in_restrict and &#39;upgrade&#39; not in group_by_names:
            logger.info(f&#34;Restricting query to Upgrade {upgrade_id}.&#34;)
            restrict.append((self._bsq.ts_table.c[&#39;upgrade&#39;], [upgrade_id]))

        query = self._bsq._add_restrict(query, restrict)
        query = self._bsq._add_group_by(query, group_by_selection)
        query = self._bsq._add_order_by(query, group_by_selection if sort else [])
        query = query.limit(limit) if limit else query

        if get_query_only:
            return self._bsq._compile(query)

        return self._bsq.execute(query, run_async=run_async)

    def get_building_average_kws_at(self,
                                    at_hour: list[int],
                                    at_days: list[int],
                                    enduses: list[str] | None = None,
                                    get_query_only: bool = False):
        &#34;&#34;&#34;
        Aggregates the timeseries result on select enduses, for the given days and hours.
        If all of the hour(s) fall exactly on the simulation timestamps, the aggregation is done by averaging the kW at
        those time stamps. If any of the hour(s) fall in between timestamps, then the following process is followed:
            i. The average kWs is calculated for timestamps specified by the hour, or just after it. Call it upper_kw
            ii. The average kWs is calculated for timestamps specified by the hour, or just before it. Call it lower_kw
            iii. Return the interpolation between upper_kw and lower_kw based on the average location of the hour(s)
                 between the upper and lower timestamps.

        Check the argument description below to learn about additional features and options.
        Args:
            at_hour: the hour(s) at which the average kWs of buildings need to be calculated at. It can either be a
                     single number if the hour is same for all days, or a list of numbers if the kW needs to be
                     calculated for different hours for different days.

            at_days: The list of days (of year) for which the average kW is to be calculated for.

            enduses: The list of enduses for which to calculate the average kWs

            get_query_only: Skips submitting the query to Athena and just returns the query strings. Useful for batch
                            submitting multiple queries or debugging.

        Returns:
                If get_query_only is True, returns two queries that gets the KW at two timestamps that are to immediate
                    left and right of the the supplied hour.
                If get_query_only is False, returns the average KW of each building at the given hour(s) across the
                supplied days.

        &#34;&#34;&#34;
        if isinstance(at_hour, list):
            if len(at_hour) != len(at_days) or not at_hour:
                raise ValueError(&#34;The length of at_hour list should be the same as length of at_days list and&#34;
                                 &#34; not be empty&#34;)
        elif isinstance(at_hour, (float, int)):
            at_hour = [at_hour] * len(at_days)
        else:
            raise ValueError(&#34;At hour should be a list or a number&#34;)

        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        total_weight = self._bsq._get_weight([])

        sim_year, sim_interval_seconds = self._bsq._get_simulation_info()
        kw_factor = 3600.0 / sim_interval_seconds

        enduse_selection = [safunc.avg(enduse * total_weight * kw_factor).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduse_cols]
        grouping_metrics_selection = [safunc.sum(1).label(&#34;sample_count&#34;),
                                      safunc.sum(total_weight).label(&#34;units_count&#34;)]

        def get_upper_timestamps(day, hour):
            new_dt = datetime.datetime(year=sim_year, month=1, day=1)

            if round(hour * 3600 % sim_interval_seconds, 2) == 0:
                # if the hour falls exactly on the simulation timestamp, use the same timestamp
                # for both lower and upper
                add = 0
            else:
                add = 1

            upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                                   (int(hour * 3600 / sim_interval_seconds) + add))
            if upper_dt.year &gt; sim_year:
                upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                                       (int(hour * 3600 / sim_interval_seconds)))
            return upper_dt

        def get_lower_timestamps(day, hour):
            new_dt = datetime.datetime(year=sim_year, month=1, day=1)
            lower_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds * int(hour * 3600 /
                                                                                                sim_interval_seconds))
            return lower_dt

        # check if the supplied hours fall exactly on the simulation timestamps
        exact_times = np.all([round(h * 3600 % sim_interval_seconds, 2) == 0 for h in at_hour])
        lower_timestamps = [get_lower_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]
        upper_timestamps = [get_upper_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]

        query = sa.select([self._bsq.ts_bldgid_column] + grouping_metrics_selection + enduse_selection)
        query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
        query = self._bsq._add_group_by(query, [self._bsq.ts_bldgid_column])
        query = self._bsq._add_order_by(query, [self._bsq.ts_bldgid_column])

        lower_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, lower_timestamps)])
        upper_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, upper_timestamps)])

        if exact_times:
            # only one query is sufficient if the hours fall in exact timestamps
            queries = [lower_val_query]
        else:
            queries = [lower_val_query, upper_val_query]

        if get_query_only:
            return [self._bsq._compile(q) for q in queries]

        batch_id = self._bsq.submit_batch_query(queries)
        if exact_times:
            vals, = self._bsq.get_batch_query_result(batch_id, combine=False)
            return vals
        else:
            lower_vals, upper_vals = self._bsq.get_batch_query_result(batch_id, combine=False)
            avg_upper_weight = np.mean([min_of_hour / sim_interval_seconds for hour in at_hour if
                                        (min_of_hour := hour * 3600 % sim_interval_seconds)])
            avg_lower_weight = 1 - avg_upper_weight
            # modify the lower vals to make it weighted average of upper and lower vals
            lower_vals[enduses] = lower_vals[enduses] * avg_lower_weight + upper_vals[enduses] * avg_upper_weight
            return lower_vals</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="buildstock_query.aggregate_query.BuildStockAggregate"><code class="flex name class">
<span>class <span class="ident">BuildStockAggregate</span></span>
<span>(</span><span>buildstock_query: main.BuildStockQuery)</span>
</code></dt>
<dd>
<div class="desc"><p>A class to perform various aggregation queries for both timeseries and annual results.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BuildStockAggregate:
    &#34;&#34;&#34;A class to perform various aggregation queries for both timeseries and annual results.
    &#34;&#34;&#34;
    def __init__(self, buildstock_query: &#39;main.BuildStockQuery&#39;) -&gt; None:
        self._bsq = buildstock_query

    def aggregate_annual(self,
                         enduses: list[str] | None = None,
                         group_by: list[str] | None = None,
                         sort: bool = False,
                         upgrade_id: str | int | None = None,
                         join_list: list[tuple[str, str, str]] | None = None,
                         weights: list[str | tuple] | None = None,
                         restrict: list[tuple[str, list]] | None = None,
                         get_quartiles: bool = False,
                         run_async: bool = False,
                         get_query_only: bool = False):
        &#34;&#34;&#34;
        Aggregates the baseline annual result on select enduses.
        Check the argument description below to learn about additional features and options.
        Args:
            enduses: The list of enduses to aggregate. Defaults to all electricity enduses

            group_by: The list of columns to group the aggregation by.

            sort: Whether to sort the results by group_by colummns

            upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

            join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                       `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                       tuples.
                       Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                                where baseline_column_name and new_column_name are the columns on which the new_table
                                should be joined to baseline table.

            weights: The additional columns to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.
                     It is specified as either list of string or list of tuples. When only string is used, the string
                     is the column name, when tuple is passed, the second element is the table name.

            restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                      Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
            get_quartiles: If true, return the following quartiles in addition to the sum for each enduses:
                           [0, 0.02, .25, .5, .75, .98, 1]. The 0% quartile is the minimum and the 100% quartile
                           is the maximum.
            run_async: Whether to run the query in the background. Returns immediately if running in background,
                       blocks otherwise.
            get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                            submitting multiple queries or debugging

        Returns:
                if get_query_only is True, returns the query_string, otherwise,
                    if run_async is True, it returns a query_execution_id.
                    if run_async is False, it returns the result_dataframe

        &#34;&#34;&#34;
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []

        [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
        if upgrade_id in {None, 0, &#39;0&#39;}:
            enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;baseline&#39;)
        else:
            upgrade_id = self._bsq._validate_upgrade(upgrade_id)
            enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;upgrade&#39;)

        total_weight = self._bsq._get_weight(weights)
        enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduse_cols]
        if get_quartiles:
            enduse_selection += [sa.func.approx_percentile(enduse, [0, 0.02, 0.25, 0.5, 0.75, 0.98, 1]).label(
                f&#34;{self._bsq._simple_label(enduse.name)}__quartiles&#34;) for enduse in enduse_cols]

        grouping_metrics_selction = [safunc.sum(1).label(&#34;sample_count&#34;),
                                     safunc.sum(total_weight).label(&#34;units_count&#34;)]

        if not group_by:
            query = sa.select(grouping_metrics_selction + enduse_selection)
            group_by_selection = []
        else:
            group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=True)
            query = sa.select(group_by_selection + grouping_metrics_selction + enduse_selection)
        # jj = self.bs_table.join(self.ts_table, self.ts_table.c[&#39;building_id&#39;]==self.bs_table.c[&#39;building_id&#39;])
        # self._compile(query.select_from(jj))
        if upgrade_id not in [None, 0, &#39;0&#39;]:
            tbljoin = self._bsq.bs_table.join(
                self._bsq.up_table, sa.and_(self._bsq.bs_table.c[self._bsq.building_id_column_name] ==
                                            self._bsq.up_table.c[self._bsq.building_id_column_name],
                                            self._bsq.up_table.c[&#34;upgrade&#34;] == str(upgrade_id),
                                            self._bsq.up_table.c[&#34;completed_status&#34;] == &#34;Success&#34;))
            query = query.select_from(tbljoin)

        restrict = [(self._bsq.bs_table.c[&#39;completed_status&#39;], [&#39;Success&#39;])] + restrict
        query = self._bsq._add_join(query, join_list)
        query = self._bsq._add_restrict(query, restrict)
        query = self._bsq._add_group_by(query, group_by_selection)
        query = self._bsq._add_order_by(query, group_by_selection if sort else [])

        if get_query_only:
            return self._bsq._compile(query)

        return self._bsq.execute(query, run_async=run_async)

    def _aggregate_timeseries_light(self,
                                    enduses: list[str] | None = None,
                                    group_by: list[str] | None = None,
                                    sort: bool = False,
                                    join_list: list[tuple[str, str, str]] | None = None,
                                    weights: list[str] | None = None,
                                    restrict: list[tuple[str, list]] | None = None,
                                    run_async: bool = False,
                                    get_query_only: bool = False,
                                    limit=None
                                    ):
        &#34;&#34;&#34;
        Lighter version of aggregate_timeseries where each enduse is submitted as a separate query to be light on
        Athena. For information on the input parameters, check the documentation on aggregate_timeseries.
        &#34;&#34;&#34;
        enduses = list(enduses) if enduses else []
        group_by = list(group_by) if group_by else []
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []

        if run_async:
            raise ValueError(&#34;Async run is not available for aggregate_timeseries_light since it needs to combine&#34;
                             &#34;the result after the query finishes.&#34;)

        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        print(enduses)
        batch_queries_to_submit = []
        for enduse in enduse_cols:
            query = self.aggregate_timeseries(enduses=[enduse.name],
                                              group_by=group_by,
                                              sort=sort,
                                              join_list=join_list,
                                              weights=weights,
                                              restrict=restrict,
                                              get_query_only=True,
                                              limit=limit)
            batch_queries_to_submit.append(query)

        if get_query_only:
            logger.warning(&#34;Not recommended to use get_query_only and split_enduses used together.&#34;
                           &#34; The results from the queries cannot be directly combined to get the desired result.&#34;
                           &#34; There are further processing done in the function. The queries should be used for&#34;
                           &#34; information or debugging purpose only. Use get_query_only=False to get proper result.&#34;)
            return batch_queries_to_submit

        batch_query_id = self._bsq.submit_batch_query(batch_queries_to_submit)

        result_dfs = self._bsq.get_batch_query_result(batch_id=batch_query_id, combine=False)
        logger.info(&#34;Joining the individual enduses result into a single DataFrame&#34;)
        group_by = self._bsq._clean_group_by(group_by)
        for res in result_dfs:
            res.set_index(group_by, inplace=True)
        self.result_dfs = result_dfs
        joined_enduses_df = result_dfs[0].drop(columns=[&#39;query_id&#39;])
        for enduse, res in list(zip(enduses, result_dfs))[1:]:
            joined_enduses_df = joined_enduses_df.join(res[[enduse.name]])

        logger.info(&#34;Joining Completed.&#34;)
        return joined_enduses_df.reset_index()

    def aggregate_timeseries(self,
                             enduses: list[str] | None = None,
                             group_by: list[str] | None = None,
                             upgrade_id: int | None = None,
                             sort: bool = False,
                             join_list: list[tuple[str, str, str]] | None = None,
                             weights: list[str] | None = None,
                             restrict: list[tuple[str, list]] | None = None,
                             run_async: bool = False,
                             split_enduses: bool = False,
                             collapse_ts: bool = False,
                             get_query_only: bool = False,
                             limit: int | None = None
                             ):
        &#34;&#34;&#34;
        Aggregates the timeseries result on select enduses.
        Check the argument description below to learn about additional features and options.
        Args:
            enduses: The list of enduses to aggregate. Defaults to all electricity enduses

            group_by: The list of columns to group the aggregation by.

            upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

            order_by: The columns by which to sort the result.

            join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                       `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                       tuples.
                       Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                                where baseline_column_name and new_column_name are the columns on which the new_table
                                should be joined to baseline table.

            weights: The additional column to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.

            restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                      Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
            limit: The maximum number of rows to query

            run_async: Whether to run the query in the background. Returns immediately if running in background,
                       blocks otherwise.
            split_enduses: Whether to query for each enduses in a separate query to reduce Athena load for query. Useful
                           when Athena runs into &#34;Query exhausted resources ...&#34; errors.
            get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                            submitting multiple queries or debugging


        Returns:
                if get_query_only is True, returns the query_string, otherwise,
                    if run_async is True, it returns a query_execution_id.
                    if run_async is False, it returns the result_dataframe

        &#34;&#34;&#34;
        enduses = list(enduses) if enduses else []
        group_by = list(group_by) if group_by else []
        join_list = list(join_list) if join_list else []
        weights = list(weights) if weights else []
        restrict = list(restrict) if restrict else []
        upgrade_id = self._bsq._validate_upgrade(upgrade_id)

        if split_enduses:
            return self._aggregate_timeseries_light(enduses=enduses, group_by=group_by, sort=sort,
                                                    join_list=join_list, weights=weights, restrict=restrict,
                                                    run_async=run_async, get_query_only=get_query_only,
                                                    limit=limit)
        [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
        enduses_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        total_weight = self._bsq._get_weight(weights)

        enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduses_cols]

        if self._bsq.timestamp_column_name not in group_by and collapse_ts:
            logger.info(&#34;Aggregation done accross timestamps. Result no longer a timeseries.&#34;)
            # The aggregation is done across time so we should correct sample_count and units_count
            rows_per_building = self._bsq._get_rows_per_building()
            grouping_metrics_selection = [(safunc.sum(1) / rows_per_building).label(
                &#34;sample_count&#34;), safunc.sum(total_weight / rows_per_building).label(&#34;units_count&#34;)]
        elif self._bsq.timestamp_column_name not in group_by:
            group_by.append(self._bsq.timestamp_column_name)
            grouping_metrics_selection = [safunc.sum(1).label(
                &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
        elif collapse_ts:
            raise ValueError(&#34;collapse_ts is true, but there is timestamp column in group_by.&#34;)
        else:
            grouping_metrics_selection = [safunc.sum(1).label(
                &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
        group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=False)

        query = sa.select(group_by_selection + grouping_metrics_selection + enduse_selection)
        query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
        if join_list:
            query = self._bsq._add_join(query, join_list)

        group_by_names = [g.name for g in group_by_selection]
        upgrade_in_restrict = any(entry[0] == &#39;upgrade&#39; for entry in restrict)
        if self._bsq.up_table is not None and not upgrade_in_restrict and &#39;upgrade&#39; not in group_by_names:
            logger.info(f&#34;Restricting query to Upgrade {upgrade_id}.&#34;)
            restrict.append((self._bsq.ts_table.c[&#39;upgrade&#39;], [upgrade_id]))

        query = self._bsq._add_restrict(query, restrict)
        query = self._bsq._add_group_by(query, group_by_selection)
        query = self._bsq._add_order_by(query, group_by_selection if sort else [])
        query = query.limit(limit) if limit else query

        if get_query_only:
            return self._bsq._compile(query)

        return self._bsq.execute(query, run_async=run_async)

    def get_building_average_kws_at(self,
                                    at_hour: list[int],
                                    at_days: list[int],
                                    enduses: list[str] | None = None,
                                    get_query_only: bool = False):
        &#34;&#34;&#34;
        Aggregates the timeseries result on select enduses, for the given days and hours.
        If all of the hour(s) fall exactly on the simulation timestamps, the aggregation is done by averaging the kW at
        those time stamps. If any of the hour(s) fall in between timestamps, then the following process is followed:
            i. The average kWs is calculated for timestamps specified by the hour, or just after it. Call it upper_kw
            ii. The average kWs is calculated for timestamps specified by the hour, or just before it. Call it lower_kw
            iii. Return the interpolation between upper_kw and lower_kw based on the average location of the hour(s)
                 between the upper and lower timestamps.

        Check the argument description below to learn about additional features and options.
        Args:
            at_hour: the hour(s) at which the average kWs of buildings need to be calculated at. It can either be a
                     single number if the hour is same for all days, or a list of numbers if the kW needs to be
                     calculated for different hours for different days.

            at_days: The list of days (of year) for which the average kW is to be calculated for.

            enduses: The list of enduses for which to calculate the average kWs

            get_query_only: Skips submitting the query to Athena and just returns the query strings. Useful for batch
                            submitting multiple queries or debugging.

        Returns:
                If get_query_only is True, returns two queries that gets the KW at two timestamps that are to immediate
                    left and right of the the supplied hour.
                If get_query_only is False, returns the average KW of each building at the given hour(s) across the
                supplied days.

        &#34;&#34;&#34;
        if isinstance(at_hour, list):
            if len(at_hour) != len(at_days) or not at_hour:
                raise ValueError(&#34;The length of at_hour list should be the same as length of at_days list and&#34;
                                 &#34; not be empty&#34;)
        elif isinstance(at_hour, (float, int)):
            at_hour = [at_hour] * len(at_days)
        else:
            raise ValueError(&#34;At hour should be a list or a number&#34;)

        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
        total_weight = self._bsq._get_weight([])

        sim_year, sim_interval_seconds = self._bsq._get_simulation_info()
        kw_factor = 3600.0 / sim_interval_seconds

        enduse_selection = [safunc.avg(enduse * total_weight * kw_factor).label(self._bsq._simple_label(enduse.name))
                            for enduse in enduse_cols]
        grouping_metrics_selection = [safunc.sum(1).label(&#34;sample_count&#34;),
                                      safunc.sum(total_weight).label(&#34;units_count&#34;)]

        def get_upper_timestamps(day, hour):
            new_dt = datetime.datetime(year=sim_year, month=1, day=1)

            if round(hour * 3600 % sim_interval_seconds, 2) == 0:
                # if the hour falls exactly on the simulation timestamp, use the same timestamp
                # for both lower and upper
                add = 0
            else:
                add = 1

            upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                                   (int(hour * 3600 / sim_interval_seconds) + add))
            if upper_dt.year &gt; sim_year:
                upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                                       (int(hour * 3600 / sim_interval_seconds)))
            return upper_dt

        def get_lower_timestamps(day, hour):
            new_dt = datetime.datetime(year=sim_year, month=1, day=1)
            lower_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds * int(hour * 3600 /
                                                                                                sim_interval_seconds))
            return lower_dt

        # check if the supplied hours fall exactly on the simulation timestamps
        exact_times = np.all([round(h * 3600 % sim_interval_seconds, 2) == 0 for h in at_hour])
        lower_timestamps = [get_lower_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]
        upper_timestamps = [get_upper_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]

        query = sa.select([self._bsq.ts_bldgid_column] + grouping_metrics_selection + enduse_selection)
        query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
        query = self._bsq._add_group_by(query, [self._bsq.ts_bldgid_column])
        query = self._bsq._add_order_by(query, [self._bsq.ts_bldgid_column])

        lower_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, lower_timestamps)])
        upper_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, upper_timestamps)])

        if exact_times:
            # only one query is sufficient if the hours fall in exact timestamps
            queries = [lower_val_query]
        else:
            queries = [lower_val_query, upper_val_query]

        if get_query_only:
            return [self._bsq._compile(q) for q in queries]

        batch_id = self._bsq.submit_batch_query(queries)
        if exact_times:
            vals, = self._bsq.get_batch_query_result(batch_id, combine=False)
            return vals
        else:
            lower_vals, upper_vals = self._bsq.get_batch_query_result(batch_id, combine=False)
            avg_upper_weight = np.mean([min_of_hour / sim_interval_seconds for hour in at_hour if
                                        (min_of_hour := hour * 3600 % sim_interval_seconds)])
            avg_lower_weight = 1 - avg_upper_weight
            # modify the lower vals to make it weighted average of upper and lower vals
            lower_vals[enduses] = lower_vals[enduses] * avg_lower_weight + upper_vals[enduses] * avg_upper_weight
            return lower_vals</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="buildstock_query.aggregate_query.BuildStockAggregate.aggregate_annual"><code class="name flex">
<span>def <span class="ident">aggregate_annual</span></span>(<span>self, enduses: list[str] | None = None, group_by: list[str] | None = None, sort: bool = False, upgrade_id: str | int | None = None, join_list: list[tuple[str, str, str]] | None = None, weights: list[str | tuple] | None = None, restrict: list[tuple[str, list]] | None = None, get_quartiles: bool = False, run_async: bool = False, get_query_only: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregates the baseline annual result on select enduses.
Check the argument description below to learn about additional features and options.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>enduses</code></strong></dt>
<dd>The list of enduses to aggregate. Defaults to all electricity enduses</dd>
<dt><strong><code>group_by</code></strong></dt>
<dd>The list of columns to group the aggregation by.</dd>
<dt><strong><code>sort</code></strong></dt>
<dd>Whether to sort the results by group_by colummns</dd>
<dt><strong><code>upgrade_id</code></strong></dt>
<dd>The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline</dd>
<dt><strong><code>join_list</code></strong></dt>
<dd>Additional table to join to baseline table to perform operation. All the inputs (<code>enduses</code>,
<code>group_by</code> etc) can use columns from these additional tables. It should be specified as a list of
tuples.
Example: <code>[(new_table_name, baseline_column_name, new_column_name), &hellip;]</code>
where baseline_column_name and new_column_name are the columns on which the new_table
should be joined to baseline table.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The additional columns to use as weight. The "build_existing_model.sample_weight" is already used.
It is specified as either list of string or list of tuples. When only string is used, the string
is the column name, when tuple is passed, the second element is the table name.</dd>
<dt><strong><code>restrict</code></strong></dt>
<dd>The list of where condition to restrict the results to. It should be specified as a list of tuple.
Example: <code>[('state',['VA','AZ']), ("build_existing_model.lighting",['60% CFL']), ...]</code></dd>
<dt><strong><code>get_quartiles</code></strong></dt>
<dd>If true, return the following quartiles in addition to the sum for each enduses:
[0, 0.02, .25, .5, .75, .98, 1]. The 0% quartile is the minimum and the 100% quartile
is the maximum.</dd>
<dt><strong><code>run_async</code></strong></dt>
<dd>Whether to run the query in the background. Returns immediately if running in background,
blocks otherwise.</dd>
<dt><strong><code>get_query_only</code></strong></dt>
<dd>Skips submitting the query to Athena and just returns the query string. Useful for batch
submitting multiple queries or debugging</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>if get_query_only is True, returns the query_string, otherwise,
if run_async is True, it returns a query_execution_id.
if run_async is False, it returns the result_dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aggregate_annual(self,
                     enduses: list[str] | None = None,
                     group_by: list[str] | None = None,
                     sort: bool = False,
                     upgrade_id: str | int | None = None,
                     join_list: list[tuple[str, str, str]] | None = None,
                     weights: list[str | tuple] | None = None,
                     restrict: list[tuple[str, list]] | None = None,
                     get_quartiles: bool = False,
                     run_async: bool = False,
                     get_query_only: bool = False):
    &#34;&#34;&#34;
    Aggregates the baseline annual result on select enduses.
    Check the argument description below to learn about additional features and options.
    Args:
        enduses: The list of enduses to aggregate. Defaults to all electricity enduses

        group_by: The list of columns to group the aggregation by.

        sort: Whether to sort the results by group_by colummns

        upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

        join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                   `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                   tuples.
                   Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                            where baseline_column_name and new_column_name are the columns on which the new_table
                            should be joined to baseline table.

        weights: The additional columns to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.
                 It is specified as either list of string or list of tuples. When only string is used, the string
                 is the column name, when tuple is passed, the second element is the table name.

        restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                  Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
        get_quartiles: If true, return the following quartiles in addition to the sum for each enduses:
                       [0, 0.02, .25, .5, .75, .98, 1]. The 0% quartile is the minimum and the 100% quartile
                       is the maximum.
        run_async: Whether to run the query in the background. Returns immediately if running in background,
                   blocks otherwise.
        get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                        submitting multiple queries or debugging

    Returns:
            if get_query_only is True, returns the query_string, otherwise,
                if run_async is True, it returns a query_execution_id.
                if run_async is False, it returns the result_dataframe

    &#34;&#34;&#34;
    join_list = list(join_list) if join_list else []
    weights = list(weights) if weights else []
    restrict = list(restrict) if restrict else []

    [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
    if upgrade_id in {None, 0, &#39;0&#39;}:
        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;baseline&#39;)
    else:
        upgrade_id = self._bsq._validate_upgrade(upgrade_id)
        enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;upgrade&#39;)

    total_weight = self._bsq._get_weight(weights)
    enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                        for enduse in enduse_cols]
    if get_quartiles:
        enduse_selection += [sa.func.approx_percentile(enduse, [0, 0.02, 0.25, 0.5, 0.75, 0.98, 1]).label(
            f&#34;{self._bsq._simple_label(enduse.name)}__quartiles&#34;) for enduse in enduse_cols]

    grouping_metrics_selction = [safunc.sum(1).label(&#34;sample_count&#34;),
                                 safunc.sum(total_weight).label(&#34;units_count&#34;)]

    if not group_by:
        query = sa.select(grouping_metrics_selction + enduse_selection)
        group_by_selection = []
    else:
        group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=True)
        query = sa.select(group_by_selection + grouping_metrics_selction + enduse_selection)
    # jj = self.bs_table.join(self.ts_table, self.ts_table.c[&#39;building_id&#39;]==self.bs_table.c[&#39;building_id&#39;])
    # self._compile(query.select_from(jj))
    if upgrade_id not in [None, 0, &#39;0&#39;]:
        tbljoin = self._bsq.bs_table.join(
            self._bsq.up_table, sa.and_(self._bsq.bs_table.c[self._bsq.building_id_column_name] ==
                                        self._bsq.up_table.c[self._bsq.building_id_column_name],
                                        self._bsq.up_table.c[&#34;upgrade&#34;] == str(upgrade_id),
                                        self._bsq.up_table.c[&#34;completed_status&#34;] == &#34;Success&#34;))
        query = query.select_from(tbljoin)

    restrict = [(self._bsq.bs_table.c[&#39;completed_status&#39;], [&#39;Success&#39;])] + restrict
    query = self._bsq._add_join(query, join_list)
    query = self._bsq._add_restrict(query, restrict)
    query = self._bsq._add_group_by(query, group_by_selection)
    query = self._bsq._add_order_by(query, group_by_selection if sort else [])

    if get_query_only:
        return self._bsq._compile(query)

    return self._bsq.execute(query, run_async=run_async)</code></pre>
</details>
</dd>
<dt id="buildstock_query.aggregate_query.BuildStockAggregate.aggregate_timeseries"><code class="name flex">
<span>def <span class="ident">aggregate_timeseries</span></span>(<span>self, enduses: list[str] | None = None, group_by: list[str] | None = None, upgrade_id: int | None = None, sort: bool = False, join_list: list[tuple[str, str, str]] | None = None, weights: list[str] | None = None, restrict: list[tuple[str, list]] | None = None, run_async: bool = False, split_enduses: bool = False, collapse_ts: bool = False, get_query_only: bool = False, limit: int | None = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregates the timeseries result on select enduses.
Check the argument description below to learn about additional features and options.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>enduses</code></strong></dt>
<dd>The list of enduses to aggregate. Defaults to all electricity enduses</dd>
<dt><strong><code>group_by</code></strong></dt>
<dd>The list of columns to group the aggregation by.</dd>
<dt><strong><code>upgrade_id</code></strong></dt>
<dd>The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline</dd>
<dt><strong><code>order_by</code></strong></dt>
<dd>The columns by which to sort the result.</dd>
<dt><strong><code>join_list</code></strong></dt>
<dd>Additional table to join to baseline table to perform operation. All the inputs (<code>enduses</code>,
<code>group_by</code> etc) can use columns from these additional tables. It should be specified as a list of
tuples.
Example: <code>[(new_table_name, baseline_column_name, new_column_name), &hellip;]</code>
where baseline_column_name and new_column_name are the columns on which the new_table
should be joined to baseline table.</dd>
<dt><strong><code>weights</code></strong></dt>
<dd>The additional column to use as weight. The "build_existing_model.sample_weight" is already used.</dd>
<dt><strong><code>restrict</code></strong></dt>
<dd>The list of where condition to restrict the results to. It should be specified as a list of tuple.
Example: <code>[('state',['VA','AZ']), ("build_existing_model.lighting",['60% CFL']), ...]</code></dd>
<dt><strong><code>limit</code></strong></dt>
<dd>The maximum number of rows to query</dd>
<dt><strong><code>run_async</code></strong></dt>
<dd>Whether to run the query in the background. Returns immediately if running in background,
blocks otherwise.</dd>
<dt><strong><code>split_enduses</code></strong></dt>
<dd>Whether to query for each enduses in a separate query to reduce Athena load for query. Useful
when Athena runs into "Query exhausted resources &hellip;" errors.</dd>
<dt><strong><code>get_query_only</code></strong></dt>
<dd>Skips submitting the query to Athena and just returns the query string. Useful for batch
submitting multiple queries or debugging</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>if get_query_only is True, returns the query_string, otherwise,
if run_async is True, it returns a query_execution_id.
if run_async is False, it returns the result_dataframe</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aggregate_timeseries(self,
                         enduses: list[str] | None = None,
                         group_by: list[str] | None = None,
                         upgrade_id: int | None = None,
                         sort: bool = False,
                         join_list: list[tuple[str, str, str]] | None = None,
                         weights: list[str] | None = None,
                         restrict: list[tuple[str, list]] | None = None,
                         run_async: bool = False,
                         split_enduses: bool = False,
                         collapse_ts: bool = False,
                         get_query_only: bool = False,
                         limit: int | None = None
                         ):
    &#34;&#34;&#34;
    Aggregates the timeseries result on select enduses.
    Check the argument description below to learn about additional features and options.
    Args:
        enduses: The list of enduses to aggregate. Defaults to all electricity enduses

        group_by: The list of columns to group the aggregation by.

        upgrade_id: The upgrade to query for. Only valid with runs with upgrade. If not provided, use the baseline

        order_by: The columns by which to sort the result.

        join_list: Additional table to join to baseline table to perform operation. All the inputs (`enduses`,
                   `group_by` etc) can use columns from these additional tables. It should be specified as a list of
                   tuples.
                   Example: `[(new_table_name, baseline_column_name, new_column_name), ...]`
                            where baseline_column_name and new_column_name are the columns on which the new_table
                            should be joined to baseline table.

        weights: The additional column to use as weight. The &#34;build_existing_model.sample_weight&#34; is already used.

        restrict: The list of where condition to restrict the results to. It should be specified as a list of tuple.
                  Example: `[(&#39;state&#39;,[&#39;VA&#39;,&#39;AZ&#39;]), (&#34;build_existing_model.lighting&#34;,[&#39;60% CFL&#39;]), ...]`
        limit: The maximum number of rows to query

        run_async: Whether to run the query in the background. Returns immediately if running in background,
                   blocks otherwise.
        split_enduses: Whether to query for each enduses in a separate query to reduce Athena load for query. Useful
                       when Athena runs into &#34;Query exhausted resources ...&#34; errors.
        get_query_only: Skips submitting the query to Athena and just returns the query string. Useful for batch
                        submitting multiple queries or debugging


    Returns:
            if get_query_only is True, returns the query_string, otherwise,
                if run_async is True, it returns a query_execution_id.
                if run_async is False, it returns the result_dataframe

    &#34;&#34;&#34;
    enduses = list(enduses) if enduses else []
    group_by = list(group_by) if group_by else []
    join_list = list(join_list) if join_list else []
    weights = list(weights) if weights else []
    restrict = list(restrict) if restrict else []
    upgrade_id = self._bsq._validate_upgrade(upgrade_id)

    if split_enduses:
        return self._aggregate_timeseries_light(enduses=enduses, group_by=group_by, sort=sort,
                                                join_list=join_list, weights=weights, restrict=restrict,
                                                run_async=run_async, get_query_only=get_query_only,
                                                limit=limit)
    [self._bsq.get_table(jl[0]) for jl in join_list]  # ingress all tables in join list
    enduses_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
    total_weight = self._bsq._get_weight(weights)

    enduse_selection = [safunc.sum(enduse * total_weight).label(self._bsq._simple_label(enduse.name))
                        for enduse in enduses_cols]

    if self._bsq.timestamp_column_name not in group_by and collapse_ts:
        logger.info(&#34;Aggregation done accross timestamps. Result no longer a timeseries.&#34;)
        # The aggregation is done across time so we should correct sample_count and units_count
        rows_per_building = self._bsq._get_rows_per_building()
        grouping_metrics_selection = [(safunc.sum(1) / rows_per_building).label(
            &#34;sample_count&#34;), safunc.sum(total_weight / rows_per_building).label(&#34;units_count&#34;)]
    elif self._bsq.timestamp_column_name not in group_by:
        group_by.append(self._bsq.timestamp_column_name)
        grouping_metrics_selection = [safunc.sum(1).label(
            &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
    elif collapse_ts:
        raise ValueError(&#34;collapse_ts is true, but there is timestamp column in group_by.&#34;)
    else:
        grouping_metrics_selection = [safunc.sum(1).label(
            &#34;sample_count&#34;), safunc.sum(total_weight).label(&#34;units_count&#34;)]
    group_by_selection = self._bsq._process_groupby_cols(group_by, annual_only=False)

    query = sa.select(group_by_selection + grouping_metrics_selection + enduse_selection)
    query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
    if join_list:
        query = self._bsq._add_join(query, join_list)

    group_by_names = [g.name for g in group_by_selection]
    upgrade_in_restrict = any(entry[0] == &#39;upgrade&#39; for entry in restrict)
    if self._bsq.up_table is not None and not upgrade_in_restrict and &#39;upgrade&#39; not in group_by_names:
        logger.info(f&#34;Restricting query to Upgrade {upgrade_id}.&#34;)
        restrict.append((self._bsq.ts_table.c[&#39;upgrade&#39;], [upgrade_id]))

    query = self._bsq._add_restrict(query, restrict)
    query = self._bsq._add_group_by(query, group_by_selection)
    query = self._bsq._add_order_by(query, group_by_selection if sort else [])
    query = query.limit(limit) if limit else query

    if get_query_only:
        return self._bsq._compile(query)

    return self._bsq.execute(query, run_async=run_async)</code></pre>
</details>
</dd>
<dt id="buildstock_query.aggregate_query.BuildStockAggregate.get_building_average_kws_at"><code class="name flex">
<span>def <span class="ident">get_building_average_kws_at</span></span>(<span>self, at_hour: list[int], at_days: list[int], enduses: list[str] | None = None, get_query_only: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Aggregates the timeseries result on select enduses, for the given days and hours.
If all of the hour(s) fall exactly on the simulation timestamps, the aggregation is done by averaging the kW at
those time stamps. If any of the hour(s) fall in between timestamps, then the following process is followed:
i. The average kWs is calculated for timestamps specified by the hour, or just after it. Call it upper_kw
ii. The average kWs is calculated for timestamps specified by the hour, or just before it. Call it lower_kw
iii. Return the interpolation between upper_kw and lower_kw based on the average location of the hour(s)
between the upper and lower timestamps.</p>
<p>Check the argument description below to learn about additional features and options.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>at_hour</code></strong></dt>
<dd>the hour(s) at which the average kWs of buildings need to be calculated at. It can either be a
single number if the hour is same for all days, or a list of numbers if the kW needs to be
calculated for different hours for different days.</dd>
<dt><strong><code>at_days</code></strong></dt>
<dd>The list of days (of year) for which the average kW is to be calculated for.</dd>
<dt><strong><code>enduses</code></strong></dt>
<dd>The list of enduses for which to calculate the average kWs</dd>
<dt><strong><code>get_query_only</code></strong></dt>
<dd>Skips submitting the query to Athena and just returns the query strings. Useful for batch
submitting multiple queries or debugging.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>If get_query_only is True, returns two queries that gets the KW at two timestamps that are to immediate
left and right of the the supplied hour.
If get_query_only is False, returns the average KW of each building at the given hour(s) across the
supplied days.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_building_average_kws_at(self,
                                at_hour: list[int],
                                at_days: list[int],
                                enduses: list[str] | None = None,
                                get_query_only: bool = False):
    &#34;&#34;&#34;
    Aggregates the timeseries result on select enduses, for the given days and hours.
    If all of the hour(s) fall exactly on the simulation timestamps, the aggregation is done by averaging the kW at
    those time stamps. If any of the hour(s) fall in between timestamps, then the following process is followed:
        i. The average kWs is calculated for timestamps specified by the hour, or just after it. Call it upper_kw
        ii. The average kWs is calculated for timestamps specified by the hour, or just before it. Call it lower_kw
        iii. Return the interpolation between upper_kw and lower_kw based on the average location of the hour(s)
             between the upper and lower timestamps.

    Check the argument description below to learn about additional features and options.
    Args:
        at_hour: the hour(s) at which the average kWs of buildings need to be calculated at. It can either be a
                 single number if the hour is same for all days, or a list of numbers if the kW needs to be
                 calculated for different hours for different days.

        at_days: The list of days (of year) for which the average kW is to be calculated for.

        enduses: The list of enduses for which to calculate the average kWs

        get_query_only: Skips submitting the query to Athena and just returns the query strings. Useful for batch
                        submitting multiple queries or debugging.

    Returns:
            If get_query_only is True, returns two queries that gets the KW at two timestamps that are to immediate
                left and right of the the supplied hour.
            If get_query_only is False, returns the average KW of each building at the given hour(s) across the
            supplied days.

    &#34;&#34;&#34;
    if isinstance(at_hour, list):
        if len(at_hour) != len(at_days) or not at_hour:
            raise ValueError(&#34;The length of at_hour list should be the same as length of at_days list and&#34;
                             &#34; not be empty&#34;)
    elif isinstance(at_hour, (float, int)):
        at_hour = [at_hour] * len(at_days)
    else:
        raise ValueError(&#34;At hour should be a list or a number&#34;)

    enduse_cols = self._bsq._get_enduse_cols(enduses, table=&#39;timeseries&#39;)
    total_weight = self._bsq._get_weight([])

    sim_year, sim_interval_seconds = self._bsq._get_simulation_info()
    kw_factor = 3600.0 / sim_interval_seconds

    enduse_selection = [safunc.avg(enduse * total_weight * kw_factor).label(self._bsq._simple_label(enduse.name))
                        for enduse in enduse_cols]
    grouping_metrics_selection = [safunc.sum(1).label(&#34;sample_count&#34;),
                                  safunc.sum(total_weight).label(&#34;units_count&#34;)]

    def get_upper_timestamps(day, hour):
        new_dt = datetime.datetime(year=sim_year, month=1, day=1)

        if round(hour * 3600 % sim_interval_seconds, 2) == 0:
            # if the hour falls exactly on the simulation timestamp, use the same timestamp
            # for both lower and upper
            add = 0
        else:
            add = 1

        upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                               (int(hour * 3600 / sim_interval_seconds) + add))
        if upper_dt.year &gt; sim_year:
            upper_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds *
                                                   (int(hour * 3600 / sim_interval_seconds)))
        return upper_dt

    def get_lower_timestamps(day, hour):
        new_dt = datetime.datetime(year=sim_year, month=1, day=1)
        lower_dt = new_dt + datetime.timedelta(days=day, seconds=sim_interval_seconds * int(hour * 3600 /
                                                                                            sim_interval_seconds))
        return lower_dt

    # check if the supplied hours fall exactly on the simulation timestamps
    exact_times = np.all([round(h * 3600 % sim_interval_seconds, 2) == 0 for h in at_hour])
    lower_timestamps = [get_lower_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]
    upper_timestamps = [get_upper_timestamps(d - 1, h) for d, h in zip(at_days, at_hour)]

    query = sa.select([self._bsq.ts_bldgid_column] + grouping_metrics_selection + enduse_selection)
    query = query.join(self._bsq.bs_table, self._bsq.bs_bldgid_column == self._bsq.ts_bldgid_column)
    query = self._bsq._add_group_by(query, [self._bsq.ts_bldgid_column])
    query = self._bsq._add_order_by(query, [self._bsq.ts_bldgid_column])

    lower_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, lower_timestamps)])
    upper_val_query = self._bsq._add_restrict(query, [(self._bsq.timestamp_column_name, upper_timestamps)])

    if exact_times:
        # only one query is sufficient if the hours fall in exact timestamps
        queries = [lower_val_query]
    else:
        queries = [lower_val_query, upper_val_query]

    if get_query_only:
        return [self._bsq._compile(q) for q in queries]

    batch_id = self._bsq.submit_batch_query(queries)
    if exact_times:
        vals, = self._bsq.get_batch_query_result(batch_id, combine=False)
        return vals
    else:
        lower_vals, upper_vals = self._bsq.get_batch_query_result(batch_id, combine=False)
        avg_upper_weight = np.mean([min_of_hour / sim_interval_seconds for hour in at_hour if
                                    (min_of_hour := hour * 3600 % sim_interval_seconds)])
        avg_lower_weight = 1 - avg_upper_weight
        # modify the lower vals to make it weighted average of upper and lower vals
        lower_vals[enduses] = lower_vals[enduses] * avg_lower_weight + upper_vals[enduses] * avg_upper_weight
        return lower_vals</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="buildstock_query" href="index.html">buildstock_query</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="buildstock_query.aggregate_query.BuildStockAggregate" href="#buildstock_query.aggregate_query.BuildStockAggregate">BuildStockAggregate</a></code></h4>
<ul class="">
<li><code><a title="buildstock_query.aggregate_query.BuildStockAggregate.aggregate_annual" href="#buildstock_query.aggregate_query.BuildStockAggregate.aggregate_annual">aggregate_annual</a></code></li>
<li><code><a title="buildstock_query.aggregate_query.BuildStockAggregate.aggregate_timeseries" href="#buildstock_query.aggregate_query.BuildStockAggregate.aggregate_timeseries">aggregate_timeseries</a></code></li>
<li><code><a title="buildstock_query.aggregate_query.BuildStockAggregate.get_building_average_kws_at" href="#buildstock_query.aggregate_query.BuildStockAggregate.get_building_average_kws_at">get_building_average_kws_at</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>